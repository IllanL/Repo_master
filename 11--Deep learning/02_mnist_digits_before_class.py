# -*- coding: utf-8 -*-
"""Copia de 02_mnist_digits_before_class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Sg-p7WT-MrZVzgftmSTRk_cFsHlUpbJ
"""

# Commented out IPython magic to ensure Python compatibility.
# %pylab inline
plt.style.use('seaborn-talk')

"""We will use this function to plot the metrics of our training process, to help in choosing the best model"""

def plot_metric(history, metric):
    history_dict = history.history
    values = history_dict[metric]
    if 'val_' + metric in history_dict.keys():  
        val_values = history_dict['val_' + metric]

    epochs = range(1, len(values) + 1)

    if 'val_' + metric in history_dict.keys():  
        plt.plot(epochs, val_values, label='Validation')
    plt.semilogy(epochs, values, label='Training')

    if 'val_' + metric in history_dict.keys():  
        plt.title('Training and validation %s' % metric)
    else:
        plt.title('Training %s' % metric)
    plt.xlabel('Epochs')
    plt.ylabel(metric.capitalize())
    plt.legend()
    plt.grid()

    plt.show()

"""We will use this function to explore the data"""

def plot_mnist_image(N, imgs, labels):
  print("The image below should show the number %d" % labels[N])
  plt.imshow(imgs[N,], cmap=plt.cm.binary)
  plt.grid(True)

"""# The problem

We will recognize handwritten digits. For an image containing a number between 0 and 9, we will recognize the number and will produce an int as output
"""

from keras.datasets import mnist

# Run this to download the data prior to the lecture
train_orig_data, test_orig_data = mnist.load_data()

train_imgs=train_orig_data[0]
train_labels=train_orig_data[1]

train_imgs[0]
train_labels[0]

numero=np.random.randint(0,59999)

plot_mnist_image(numero, train_imgs, train_labels)

test_imgs=test_orig_data[0]
test_labels=test_orig_data[1]

print(test_imgs.shape)
print(test_labels.shape)

train_imgs.max(), train_imgs.min()

train_imgs_sc=(train_imgs-train_imgs.max())/(train_imgs.max()-train_imgs.min())

test_imgs_sc=(test_imgs-test_imgs.max())/(test_imgs.max()-test_imgs.min())

from keras.utils import to_categorical

train_1hot=to_categorical(train_labels, num_classes=10)

train_1hot.shape

test_1hot=to_categorical(test_labels, num_classes=10)

test_1hot.shape

from keras.models import Sequential
from keras.layers import Dense, Flatten

m=Sequential()

entrada=Flatten(input_shape=(28,28)) 
m.add(entrada)

capa_oculta=Dense(units=28, input_shape=(784,), activation='relu')
m.add(capa_oculta)

capa_oculta_3=Dense (units=56,  activation='relu')
m.add(capa_oculta_3)

salida=Dense(units=10, activation='softmax')
m.add(salida)

m.summary()

from keras.optimizers import Adadelta
from keras.losses import categorical_crossentropy
from keras.metrics import mean_absolute_percentage_error
from keras.metrics import categorical_accuracy

m.compile(optimizer=Adadelta(lr=0.8),loss=categorical_crossentropy, metrics=[mean_absolute_percentage_error])

h=m.fit(x=train_imgs_sc, y=train_1hot, batch_size=100, epochs=120, validation_split=0.1)

plot_metric(h, metric='mean_absolute_percentage_error')

def distance(im1, im2):

  im1=np.reshape(im1,(784,1))
  im2=np.reshape(im2,(784,1))
  dist =1-np.linalg.norm(im1-im2)/(np.linalg.norm(im1)*np.linalg.norm(im2))

  return dist

distance(train_imgs_sc[0], train_imgs_sc[500])

clase1=5
  clase2=7
  
  aa1=train_imgs_sc[train_labels==clase1]
  aa2=train_imgs_sc[train_labels==clase2]
  aa1

aa1=train_imgs_sc[train_labels==clase1]
aa2=train_imgs_sc[train_labels==clase2]

cosa1=np.array([])
cosa2=np.array([])

for elemento in aa1:
  cosa1=np.append(cosa1, np.reshape(elemento, (784,1)))

for elemento in aa2:
  cosa2=np.append(cosa2, np.reshape(elemento, (784,1)))

matriz1=np.reshape(cosa1, (784,np.int_(len(cosa1)/784)))
matriz2=np.reshape(cosa2, (784,np.int_(len(cosa2)/784)))

matriz1.shape

matriz1[:,0:9]

def distancia2(clase1,clase2):

  aa1=train_imgs_sc[train_labels==clase1]
  aa2=train_imgs_sc[train_labels==clase2]

  cosa1=np.array([])
  cosa2=np.array([])

  for elemento in aa1:
    cosa1=np.append(cosa1, np.reshape(elemento, (784,1)))

  for elemento in aa2:
    cosa2=np.append(cosa2, np.reshape(elemento, (784,1)))

  matriz1=np.reshape(cosa1, (784,np.int_(len(cosa1)/784)))
  matriz2=np.reshape(cosa2, (784,np.int_(len(cosa2)/784)))

  d=np.dot(np.transpose(matriz1),matriz2)

  d=d.flatten()

  return d

prueba1=distancia2(8,1);
prueba2=distancia2(8,8);
prueba3=distancia2(8,0);

plt.hist(prueba2, bins=1000, color='r');
plt.hist(prueba1, bins=1000);
plt.hist(prueba3, bins=1000, color='g');

def build_regul(l2_pensalty):
  m=Sequential()

  entrada=Flatten(input_shape=(28,28)) 
  m.add(entrada)

  capa_oculta=Dense(units=28, input_shape=(784,), activation='relu', kernel_regularizer=l2(l2_pensalty))
  m.add(capa_oculta)

  capa_oculta_3=Dense (units=56,  activation='relu', kernel_regularizer=l2(l2_pensalty))
  m.add(capa_oculta_3)

  salida=Dense(units=10, activation='softmax')
  m.add(salida)
  m.compile(optimizer=Adadelta(lr=0.8),loss=categorical_crossentropy, metrics=[categorical_accuracy])

  m.summary()

modelo2=build_regul(0.2)

h=m.fit(x=train_imgs_sc, y=train_1hot, batch_size=100, epochs=30, validation_split=0.2)

plot_metric(h, 'loss')
plot_metric(h, 'categorical_accuracy')

















"""We will design a network that will be able to recognize that the image is showing the number 7 (for all the images in the test set)

## Data transform

We need to change the shape of the data, so it can be fed to the network more easily
"""



"""All the images are 28x28 matrices, with values between 0 and 255. Let's normalize the images, to avoid problems in the numerical computations using large numbers.

We will convert the matrix to a vector with 28x28 components, stacking columns on top of each other. This will make the design of the network.
* Alternatively, we could add a `Flatten(input_shape=(28,28))` layer to the model
"""



"""Because we want to assign a *category* to each image, we need to transform the output to categorical format"""



"""Initially, our target data is just a set of numbers"""



"""We are going to transform it to **1-HOT encoding format**"""



"""We have to repeat the same process with the test data"""



"""## Let's build the model"""



"""Here we will make some decisions about how to train our model.

The **objective function**, called **loss function** in Deep Learning, will be *categorical crossentropy*. This is because we are trying to predict discrete classes. If we choose a different function, the solution will be different. This is the function that will tell us when we have found the *solution*. Different functions will point to different *solutions*.

The **optimizer** is the method that we will use to find the minimum of the **loss function**. In general, we will not find the global minimum of the loss function, but a minimum that is good enough. *RMSProp* is one the variants of gradient descent. But there are many more:
 - http://ruder.io/optimizing-gradient-descent/
 
These two settings will define what solution we will find during the training process.

The other parameter, the **metrics**, is only for information purposes. In each step of the training (called **epoch**), Keras will report the value of the metrics. But whether we choose one metric or another will not influence the training process. It is only for reporting how the training is going. That information will be useful for the validation of the model, that is, for the **hyperparameters tuning**.

Now let's fit the neural network. We will keep the results in a *history* object to plot some parameters after the training, and use that information for improving our model (changing layers, activation functions, etc). This process is called **hyperparameters tuning** or hypeparameters search.

Here we need to set two parameters: the number of **epochs** and the **batch size**.

In previous cells, I have said that the training process is an optimization problem: we try to find the minimum.

That's actually not entirely true. In practice, we don't keep searching for a minimum. Instead, we train for a preset number of epochs. Then we analyze the output using the validation set. If we detect overfit, then we reduce the number of epochs. If we don't detect it, then maybe we can keep going for some more epochs to find a better model. The number of epochs is another hyperparameter. In general, we should stop the training as early as possible. Any further epoch can make our model overfit, and therefore generalize poorly.

The batch size is another hyperparameter. The network is not updated item by item, vector by vector. We can actually calculate the weights for a batch of several items, forming a matrix or a tensor. The batch size will affect the granularity of the calculations and the performance. A larger batch size will probably result in a slightly worse accuracy, but in a better performance. As long as the batch size is not so huge that it cannot be kept in memory. Because we are running on a GPU, it is a good idea to use a power of 2 for the batch size. It will probably make a better use of the GPU. But again, the effect of the batch size must be tested using the validation set, and adjusted until we find a good value for that parameter.

An epoch is a full pass over the training dataset. At the end of each epoch, Keras will calculate the value of the metrics for the training set, and the loss function and metrics for the validation set. The loss and metrics of the validation set are useful for the hyperparameters tuning process.
"""



"""## Exercises

* **EXERCISE 1**: Chante the activation function in the model. How does it affect accuracy? What happens if you use linear activation function? What happens if we use the $\tanh$ function? Does it affect to overfitting?
 * https://keras.io/activations/
 
Here we see that the accuracy keeps increasing over epochs. An overfitted model would produce a decrease of the validation accuracy at some point. This means that we have found an optimal model. The validation loss is stuck at a minimum, but the accuracy is not affected by the extra epochs.

_**EXERCISE 2**_: How is the model accuracy (measured with the test set, see below) affected if we start over and fit for 40 epochs? And for 4? And for 400?

_**EXERCISE 4**_: What is the most simple model that you can get that achieves a similar validation performance (accuracy)? (the loss values are not comparable, different networks will have different ranges of values for the loss functions)

Now let's check how well the model works with the test dataset. Remember that we don't have used it at all during the training and tuning of the model.

## Analyze the classification performance
"""



"""That's about a $98\%$ accuracy on the test set. Not bad.

With that accuracy, this is the number of images that are wrongly classified by our model:
"""



"""Let's see how well it predicts some random items from the test set."""

plot_mnist_image(2543, test_imgs, test_labels)

"""The prediction is a 10-element vector (*1-HOT encoded vector*), with the probability of each class. The location of the max is giving us the predicted class."""



"""YES! Our model predicts 8

_**EXERCISE 5**_: Can you find a test item that is predicted wrongly? How many images are predicted wrongly? Can you find all the items that are wrongly classified? For instance, the test element with index 3943 is a 3 but we predict a 5
"""

plot_mnist_image(3943, test_imgs, test_labels)

np.argmax(m.predict(test_imgs_t[3943:3944,]))



"""Ok, but in which positions can we find all the misclassifications?"""



"""_**EXERCISE 6**_: Some digits are more difficult to recognize than others. Because we know the test labels, we can find out how many times the corresponding test images are misclassified. Could you find what are the top 3 test labels that are more often misclassified?

Let's check some wrong predictions
"""

plot_mnist_image(151, test_imgs, test_labels)

preds_labels[151]

"""These are the items that are wrongly classified:"""



"""So we see that $9$ is wrongly classified more than the rest. But does it mean that our model has more difficulties recognizing 9s?"""



"""Our model has the most difficulties recognizing 9s, 5s and 4s"""

